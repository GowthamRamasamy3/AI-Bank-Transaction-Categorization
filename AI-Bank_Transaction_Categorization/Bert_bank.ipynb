{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "H0eaBHNldUgF",
        "outputId": "ab10d2f5-fc0f-4404-88a3-e1dd05b3a940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-10-c60a8e5900f9>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [76/76 31:30, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.845800</td>\n",
              "      <td>1.480700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.206500</td>\n",
              "      <td>0.388853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.422900</td>\n",
              "      <td>0.196248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.214400</td>\n",
              "      <td>0.163847</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c60a8e5900f9>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-10-c60a8e5900f9>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-10-c60a8e5900f9>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-10-c60a8e5900f9>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-10-c60a8e5900f9>:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1638467162847519, 'eval_runtime': 30.3491, 'eval_samples_per_second': 2.405, 'eval_steps_per_second': 0.066, 'epoch': 4.0}\n",
            "Accuracy: 0.9863013698630136\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2       1.00      1.00      1.00        57\n",
            "          14       0.00      0.00      0.00         1\n",
            "         169       0.94      1.00      0.97        15\n",
            "\n",
            "    accuracy                           0.99        73\n",
            "   macro avg       0.65      0.67      0.66        73\n",
            "weighted avg       0.97      0.99      0.98        73\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "#!pip install transformers torch pandas scikit-learn openpyxl\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "transactions_df = pd.read_excel('/content/Book1.xlsx')  # Ensure file path is correct\n",
        "keyword_mapping_df = pd.read_csv('/content/categories_and_keywords.csv')  # Ensure file path is correct\n",
        "\n",
        "# Prepare category mapping\n",
        "category_mapping = {category: i for i, category in enumerate(keyword_mapping_df['Category'].unique())}\n",
        "keyword_to_category = keyword_mapping_df.set_index('Keywords')['Category'].to_dict()\n",
        "\n",
        "def categorize_description(description, keyword_to_category):\n",
        "    description = description.lower()\n",
        "    for keyword, category in keyword_to_category.items():\n",
        "        if keyword in description:\n",
        "            return category\n",
        "    return 'UNKNOWN'\n",
        "\n",
        "# Add a column for initial categories based on keywords\n",
        "transactions_df['initial_category'] = transactions_df['Description'].apply(lambda x: categorize_description(x, keyword_to_category))\n",
        "\n",
        "# Include 'UNKNOWN' in the category mapping if not present\n",
        "if 'UNKNOWN' not in category_mapping:\n",
        "    new_category_id = len(category_mapping)\n",
        "    category_mapping['UNKNOWN'] = new_category_id\n",
        "\n",
        "# Update the keyword mapping with the new 'UNKNOWN' category\n",
        "keyword_mapping_df = pd.concat([\n",
        "    keyword_mapping_df,\n",
        "    pd.DataFrame({'Keyword': ['unknown'], 'Category': ['UNKNOWN']})\n",
        "], ignore_index=True)\n",
        "\n",
        "# Save updated keyword mapping\n",
        "keyword_mapping_df.to_csv('/content/updated_categories_and_keywords.csv', index=False)\n",
        "\n",
        "# Convert initial categories to numeric labels\n",
        "transactions_df['labels'] = transactions_df['initial_category'].map(category_mapping)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(category_mapping))\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Prepare text data\n",
        "texts = transactions_df['Description'].tolist()\n",
        "labels = transactions_df['labels'].tolist()\n",
        "\n",
        "# Split data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "\n",
        "# Create dataset class\n",
        "class TransactionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TransactionDataset(train_encodings, train_labels)\n",
        "test_dataset = TransactionDataset(test_encodings, test_labels)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=4,  # Increased epochs for better training\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',  # Added logging\n",
        "    logging_steps=10,      # Adjust logging steps\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(test_labels, predicted_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "transactions_path = '/content/Book1.xlsx'\n",
        "keyword_mapping_path = '/content/categories_and_keywords.csv'\n",
        "model_path = '/content/finetuned_model'\n",
        "\n",
        "# Load data\n",
        "transactions_df = pd.read_excel(transactions_path)\n",
        "keyword_mapping_df = pd.read_csv(keyword_mapping_path) if os.path.exists(keyword_mapping_path) else pd.DataFrame(columns=['Keywords', 'Category'])\n",
        "\n",
        "# Prepare category mapping\n",
        "category_mapping = {category: i for i, category in enumerate(keyword_mapping_df['Category'].unique())}\n",
        "keyword_to_category = keyword_mapping_df.set_index('Keywords')['Category'].to_dict()\n",
        "\n",
        "def categorize_description(description, keyword_to_category):\n",
        "    description = description.lower()\n",
        "    for keyword, category in keyword_to_category.items():\n",
        "        if keyword in description:\n",
        "            return category\n",
        "    return 'UNKNOWN'\n",
        "\n",
        "# Add a column for initial categories based on keywords\n",
        "transactions_df['initial_category'] = transactions_df['Description'].apply(lambda x: categorize_description(x, keyword_to_category))\n",
        "\n",
        "# Include 'UNKNOWN' in the category mapping if not present\n",
        "if 'UNKNOWN' not in category_mapping:\n",
        "    new_category_id = len(category_mapping)\n",
        "    category_mapping['UNKNOWN'] = new_category_id\n",
        "\n",
        "# Update the keyword mapping with the new 'UNKNOWN' category\n",
        "if 'UNKNOWN' not in keyword_mapping_df['Category'].values:\n",
        "    keyword_mapping_df = pd.concat([\n",
        "        keyword_mapping_df,\n",
        "        pd.DataFrame({'Keywords': ['unknown'], 'Category': ['UNKNOWN']})\n",
        "    ], ignore_index=True)\n",
        "\n",
        "keyword_mapping_df.to_csv(keyword_mapping_path, index=False)\n",
        "\n",
        "# Convert initial categories to numeric labels\n",
        "transactions_df['labels'] = transactions_df['initial_category'].map(category_mapping)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "num_labels = len(category_mapping)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Prepare text data\n",
        "texts = transactions_df['Description'].tolist()\n",
        "labels = transactions_df['labels'].tolist()\n",
        "\n",
        "# Split data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize data\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "\n",
        "# Create dataset class\n",
        "class TransactionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TransactionDataset(train_encodings, train_labels)\n",
        "test_dataset = TransactionDataset(test_encodings, test_labels)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=0.3,  # Adjust epochs as needed\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n",
        "\n",
        "def predict_and_update(description, max_attempts=8):\n",
        "    global model, category_mapping, keyword_to_category, keyword_mapping_df\n",
        "    for attempt in range(max_attempts):\n",
        "        # Check if description matches known keywords\n",
        "        for keyword, category in keyword_to_category.items():\n",
        "            if keyword in description.lower():\n",
        "                return category\n",
        "\n",
        "        # Perform prediction with BERT\n",
        "        inputs = tokenizer(description, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "        # Convert prediction id to category name\n",
        "        predicted_category = [k for k, v in category_mapping.items() if v == prediction][0]\n",
        "\n",
        "        # If prediction is UNKNOWN\n",
        "        if predicted_category == 'UNKNOWN':\n",
        "            if attempt < max_attempts - 1:\n",
        "                print(f\"Attempt {attempt + 1}/{max_attempts}: Unknown category for description: '{description}'.\")\n",
        "                new_category = input(\"Please enter the category: \").strip()\n",
        "\n",
        "                if new_category not in category_mapping:\n",
        "                    # Add new category to mappings\n",
        "                    new_category_id = len(category_mapping)\n",
        "                    category_mapping[new_category] = new_category_id\n",
        "                    keyword_to_category[description] = new_category\n",
        "                    keyword_mapping_df.loc[len(keyword_mapping_df)] = [description, new_category]\n",
        "                    keyword_mapping_df.to_csv(keyword_mapping_path, index=False)\n",
        "\n",
        "                    # Update model with new category\n",
        "                    num_labels = len(category_mapping)\n",
        "                    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "                    # Prepare updated dataset\n",
        "                    new_labels = transactions_df['labels'].tolist() + [new_category_id]\n",
        "                    new_texts = transactions_df['Description'].tolist() + [description]\n",
        "\n",
        "                    train_encodings = tokenize_function(new_texts)\n",
        "                    train_dataset = TransactionDataset(train_encodings, new_labels)\n",
        "\n",
        "                    trainer.train_dataset = train_dataset\n",
        "                    trainer.train()\n",
        "\n",
        "                return new_category\n",
        "            else:\n",
        "                return 'UNKNOWN'\n",
        "        return predicted_category\n",
        "\n",
        "# Apply predictions\n",
        "transactions_df['Category'] = transactions_df['Description'].apply(predict_and_update)\n",
        "\n",
        "# Save updated Excel file with only Category column\n",
        "transactions_df[['Category']].to_excel('/content/updated_transactions.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "yR0rf7hOcPca",
        "outputId": "dbb7b602-144c-4970-cf01-28235a503c42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-9-4d4c119c51d9>:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 07:14, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.867151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4d4c119c51d9>:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "<ipython-input-9-4d4c119c51d9>:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 3.8671514987945557, 'eval_runtime': 31.5896, 'eval_samples_per_second': 2.311, 'eval_steps_per_second': 0.063, 'epoch': 0.3157894736842105}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqpZSlUKMmDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}